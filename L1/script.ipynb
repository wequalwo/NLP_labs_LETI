{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95e16e91",
   "metadata": {},
   "source": [
    "# Лабораторная работа 1\n",
    "Выполнил студент группы 0385 Иванов Серафим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc3ffb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "import bz2\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import shutil\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6e4fee",
   "metadata": {},
   "source": [
    "## Скачивание нужных файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5851543e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файл \"opencorpora_dict.xml.bz2\" успешно скачан в \"downloads\\opencorpora_dict.xml.bz2\"\n",
      "Файл \"сrime_and_punishment.txt\" успешно скачан в \"downloads\\сrime_and_punishment.txt\"\n",
      "Файл \"war_and_peace.txt\" успешно скачан в \"downloads\\war_and_peace.txt\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Папка для сохранения файлов\n",
    "output_dir = 'downloads'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Ссылки на файлы\n",
    "urls = {\n",
    "    'opencorpora_dict.xml.bz2': 'https://opencorpora.org/files/export/dict/dict.opcorpora.xml.bz2',\n",
    "    'сrime_and_punishment.txt': 'https://bookex.info/uploads/public_files/2023-02/prestuplenie-i-nakazanie_-fedor-dostoevskij.txt',\n",
    "    'war_and_peace.txt': 'https://gist.github.com/Semionn/bdcb66640cc070450817686f6c818897/raw/f9e8c888a771dd96f54562a9b050acd1138cc7a9/war_and_peace.ru.txt'\n",
    "}\n",
    "\n",
    "# Скачивание файлов\n",
    "for filename, url in urls.items():\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    urllib.request.urlretrieve(url, output_path)\n",
    "    print(f'Файл \"{filename}\" успешно скачан в \"{output_path}\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6fca08b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file \"opencorpora_dict.xml.bz2\" mod=vet into \"opencorpora_dict.xml.bz2\"\n",
      "file \"war_and_peace.txt\" mod=vet into \"war_and_peace.txt\"\n",
      "file \"сrime_and_punishment.txt\" mod=vet into \"сrime_and_punishment.txt\"\n"
     ]
    }
   ],
   "source": [
    "# Перемещаем все файлы из downloads в родительскую папку\n",
    "parent_dir = os.path.dirname(output_dir)\n",
    "\n",
    "for filename in os.listdir(output_dir):\n",
    "    src_path = os.path.join(output_dir, filename)\n",
    "    dst_path = os.path.join(parent_dir, filename)\n",
    "    if os.path.isfile(src_path):\n",
    "        shutil.move(src_path, dst_path)\n",
    "        print(f'file \"{filename}\" mod=vet into \"{dst_path}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e571eaf9",
   "metadata": {},
   "source": [
    "## Обработка словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cabd771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BZ2_PATH = \"dict.opcorpora.xml.bz2\"   # сжатый словарь\n",
    "XML_PATH = \"dict.opcorpora.xml\"       # распакованный xml\n",
    "PICKLE_PATH = \"lemma_dict.pkl\"        # куда сохраняем готовый словарь\n",
    "\n",
    "if not os.path.exists(XML_PATH) and os.path.exists(BZ2_PATH):\n",
    "    print(\"Unpacking dict...\")\n",
    "    with bz2.open(BZ2_PATH, \"rb\") as f_in, open(XML_PATH, \"wb\") as f_out:\n",
    "        f_out.write(f_in.read())\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9435b4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAMM_TO_SIMPLE = {\n",
    "    \"NOUN\": \"S\",\n",
    "    \"S\": \"S\",\n",
    "    \"NPRO\": \"NI\",\n",
    "\n",
    "    \"ADJF\": \"A\",\n",
    "    \"ADJS\": \"A\",\n",
    "    \"COMP\": \"A\",\n",
    "\n",
    "    \"VERB\": \"V\",\n",
    "    \"INFN\": \"V\",\n",
    "    \"V\": \"V\",\n",
    "\n",
    "    \"PRTF\": \"PRTF\",  # причастие полное\n",
    "    \"PRTS\": \"PRTS\",  # причастие краткое\n",
    "    \"GRND\": \"GRND\",  # деепричастие\n",
    "\n",
    "    \"ADVB\": \"ADV\",\n",
    "    \"ADV\": \"ADV\",\n",
    "\n",
    "    \"NUMR\": \"NUM\",\n",
    "    \"NUM\": \"NUM\",\n",
    "\n",
    "    \"PREP\": \"PR\",\n",
    "    \"PR\": \"PR\",\n",
    "    \"CONJ\": \"CONJ\",\n",
    "    \"PRCL\": \"PART\",\n",
    "    \"INTJ\": \"INTJ\",\n",
    "}\n",
    "\n",
    "POS_GRAMMEMES = set(GRAMM_TO_SIMPLE.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b9723e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(word: str) -> str:\n",
    "    \"\"\"Lower case + ё->е\"\"\"\n",
    "    return word.lower().replace(\"ё\", \"е\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "05849193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting <lemma>...\n",
      "Total: 391,845\n"
     ]
    }
   ],
   "source": [
    "print(\"Counting <lemma>...\")\n",
    "lemma_count = 0\n",
    "for _, elem in ET.iterparse(XML_PATH, events=(\"end\",)):\n",
    "    if elem.tag == \"lemma\":\n",
    "        lemma_count += 1\n",
    "    elem.clear()\n",
    "print(f\"Total: {lemma_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b3ead3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing XML and collecting lemmas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391845/391845 [00:43<00:00, 9038.16lemma/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building final lemma_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060604/3060604 [00:05<00:00, 515319.57form/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dict is ready, total keys: 3060604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lemma_dict = {}\n",
    "lemma_dict_raw = {}        # lemma_id -> (lemma_text, pos_simple, forms)\n",
    "form2lemma = {}            # form_norm -> lemma_id\n",
    "links = {}                 # to_id -> from_id (связи из XML)\n",
    "\n",
    "VALID_LINK_TYPES = {\"3\", \"5\", \"6\"}  # инфинитив, деепричастие, причастие\n",
    "\n",
    "print(\"Parsing XML and collecting lemmas...\")\n",
    "it = ET.iterparse(XML_PATH, events=(\"end\",))\n",
    "with tqdm(total=lemma_count, unit=\"lemma\") as pbar:\n",
    "    for event, elem in it:\n",
    "        if elem.tag == \"lemma\":\n",
    "            lemma_id = elem.get(\"id\")\n",
    "            l_elem = elem.find(\"l\")\n",
    "            if l_elem is not None:\n",
    "                lemma_text = l_elem.get(\"t\") or \"\"\n",
    "                pos_simple = None\n",
    "                for g in l_elem.findall(\"g\"):\n",
    "                    v = g.get(\"v\")\n",
    "                    if v in POS_GRAMMEMES:\n",
    "                        pos_simple = GRAMM_TO_SIMPLE[v]\n",
    "                        break\n",
    "                if pos_simple is None:\n",
    "                    pos_simple = \"?\"\n",
    "\n",
    "                forms = [lemma_text] + [f.get(\"t\") for f in elem.findall(\"f\") if f.get(\"t\")]\n",
    "                lemma_dict_raw[lemma_id] = (lemma_text, pos_simple, forms)\n",
    "\n",
    "                for form in forms:\n",
    "                    norm = normalize(form)\n",
    "                    form2lemma[norm] = lemma_id\n",
    "\n",
    "            elem.clear()\n",
    "            pbar.update(1)\n",
    "\n",
    "        elif elem.tag == \"link\":\n",
    "            link_type = elem.get(\"type\")\n",
    "            if link_type in VALID_LINK_TYPES:\n",
    "                from_id = elem.get(\"from\")\n",
    "                to_id = elem.get(\"to\")\n",
    "                links[to_id] = from_id\n",
    "            elem.clear()\n",
    "\n",
    "# --- функция для подъема по цепочке до базовой леммы ---\n",
    "def get_base_id(lemma_id, links):\n",
    "    seen = set()\n",
    "    while lemma_id in links and lemma_id not in seen:\n",
    "        seen.add(lemma_id)\n",
    "        lemma_id = links[lemma_id]\n",
    "    return lemma_id\n",
    "\n",
    "print(\"Building final lemma_dict...\")\n",
    "with tqdm(total=len(form2lemma), unit=\"form\") as pbar:\n",
    "    for form_norm, lemma_id in form2lemma.items():\n",
    "        # POS берём от исходной формы\n",
    "        lemma_text_base, pos_simple_orig, _ = lemma_dict_raw[lemma_id]\n",
    "        # лемму берём от базовой формы по цепочке ссылок\n",
    "        base_id = get_base_id(lemma_id, links)\n",
    "        lemma_text_base = lemma_dict_raw[base_id][0]\n",
    "        lemma_norm = normalize(lemma_text_base)\n",
    "        lemma_dict[form_norm] = (lemma_norm, pos_simple_orig)\n",
    "        pbar.update(1)\n",
    "\n",
    "\n",
    "print(\"The dict is ready, total keys:\", len(lemma_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "db4eb35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to lemma_dict.pkl\n"
     ]
    }
   ],
   "source": [
    "with open(PICKLE_PATH, \"wb\") as f:\n",
    "    pickle.dump(lemma_dict, f, protocol=4)\n",
    "print(\"Saved to\", PICKLE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b76b7cb",
   "metadata": {},
   "source": [
    "## Обработка текста"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd19de85",
   "metadata": {},
   "source": [
    "Загружаем готовый словарь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "22701f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict loaded, keys: 3060604\n"
     ]
    }
   ],
   "source": [
    "PICKLE_PATH = \"lemma_dict.pkl\"\n",
    "with open(PICKLE_PATH, \"rb\") as f:\n",
    "    lemma_dict = pickle.load(f)\n",
    "print(\"Dict loaded, keys:\", len(lemma_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb95f14",
   "metadata": {},
   "source": [
    "Функции для лемматизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "26cb4277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# глобальные счётчики\n",
    "total_tokens = 0\n",
    "not_found_tokens = 0\n",
    "\n",
    "def normalize(word: str) -> str:\n",
    "    \"\"\"Normalize a token: convert to lowercase and replace 'ё' with 'е'\"\"\"\n",
    "    return word.lower().replace(\"ё\", \"е\")\n",
    "\n",
    "def lemmatize_token(token: str):\n",
    "    \"\"\"\n",
    "    Returns the token in the format: token{lemma=POS}.\n",
    "    Updates global counters:\n",
    "      - total_tokens\n",
    "      - not_found_tokens\n",
    "    \"\"\"\n",
    "    global total_tokens, not_found_tokens\n",
    "    total_tokens += 1\n",
    "\n",
    "    norm = normalize(token)\n",
    "    if norm in lemma_dict:\n",
    "        lemma, pos = lemma_dict[norm]\n",
    "        return f\"{token}{{{lemma}={pos}}}\"\n",
    "    else:\n",
    "        not_found_tokens += 1\n",
    "        return f\"{token}{{{norm}=??}}\"\n",
    "\n",
    "def process_sentence(sentence: str):\n",
    "    \"\"\"\n",
    "    1. Remove all punctuation\n",
    "    2. Split the sentence into tokens\n",
    "    3. Lemmatize each token (updates counters)\n",
    "    4. Join tokens back into a string\n",
    "    \"\"\"\n",
    "    clean = re.sub(r\"[^\\w\\s]\", \"\", sentence, flags=re.UNICODE)\n",
    "    tokens = clean.split()\n",
    "    return \" \".join(lemmatize_token(tok) for tok in tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ca653f",
   "metadata": {},
   "source": [
    "Использование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9bd7a9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "Пятеро путешественников, уставшие, но улыбающиеся, медленно поднимались по горной тропе, осторожно ступая на скользкие камни.\n",
      "Они шли, переговариваясь и смеясь, ведь впереди их ждала цель, о которой они мечтали.\n",
      "Эй, неужели дорога наконец закончилась?\n",
      "\n",
      "Output:\n",
      "Пятеро{пятеро=NUM} путешественников{путешественник=S} уставшие{уставший=PRTF} но{но=INTJ} улыбающиеся{улыбающийся=PRTF} медленно{медлен=A} поднимались{подниматься=V} по{по=S} горной{горный=A} тропе{тропа=S} осторожно{осторожен=A} ступая{ступать=GRND} на{на=PART} скользкие{скользкий=A} камни{камень=S}\n",
      "Они{они=NI} шли{слать=V} переговариваясь{переговариваться=GRND} и{и=S} смеясь{смеяться=GRND} ведь{ведь=CONJ} впереди{впереди=PR} их{они=NI} ждала{ждать=V} цель{цель=S} о{о=S} которой{который=A} они{они=NI} мечтали{мечтать=V}\n",
      "Эй{эй=INTJ} неужели{неужели=INTJ} дорога{дорог=A} наконец{наконец=CONJ} закончилась{закончиться=V}\n",
      "\n",
      "Statistics:\n",
      "Total tokens processed: 34\n",
      "Tokens not found in dictionary: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# Сброс глобальных счётчиков перед обработкой\n",
    "total_tokens = 0\n",
    "not_found_tokens = 0\n",
    "\n",
    "input_text = \"\"\"Стала стабильнее экономическая и политическая обстановка, предприятия вывели из тени зарплаты сотрудников.\n",
    "Все Гришины одноклассники уже побывали за границей, он был чуть ли не единственным, кого не вывозили никуда дальше Красной Пахры.\n",
    "\"\"\"\n",
    "\n",
    "input_text = \"\"\"Пятеро путешественников, уставшие, но улыбающиеся, медленно поднимались по горной тропе, осторожно ступая на скользкие камни.\n",
    "Они шли, переговариваясь и смеясь, ведь впереди их ждала цель, о которой они мечтали.\n",
    "Эй, неужели дорога наконец закончилась?\"\"\"\n",
    "print(\"Input:\")\n",
    "print(input_text)\n",
    "\n",
    "print(\"\\nOutput:\")\n",
    "for line in input_text.strip().split(\"\\n\"):\n",
    "    print(process_sentence(line))\n",
    "\n",
    "# Печатаем статистику\n",
    "print(\"\\nStatistics:\")\n",
    "print(f\"Total tokens processed: {total_tokens}\")\n",
    "print(f\"Tokens not found in dictionary: {not_found_tokens} ({not_found_tokens/total_tokens:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc5c71d",
   "metadata": {},
   "source": [
    "## Обработка большого текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4aa0a4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"сrime_and_punishment.txt\"   # путь к файлу с текстом\n",
    "OUTPUT_FILE = \"сrime_and_punishment_out.txt\" # куда сохранить результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4cb04bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кодировки:\n",
    "# Война и мир UTF-8\n",
    "# Преступление и наказание windows 1251\n",
    "\n",
    "# Сброс глобальных счётчиков перед обработкой\n",
    "total_tokens = 0\n",
    "not_found_tokens = 0\n",
    "\n",
    "with open(INPUT_FILE, \"r\", encoding=\"UTF-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "processed_lines = []\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if line:  # skip empty lines\n",
    "        processed_lines.append(process_sentence(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0ea3c916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed text:\n",
      "Она{она=NI} поминки{поминки=S} устраивает{устраивать=V}\n",
      "Дас{дас=??} закуску{закуска=S} она{она=NI} вас{вы=NI} очень{очень=ADV} велела{велеть=V} благодарить{благодарить=V} что{что=PART} вы{вы=NI} вчера{вчера=ADV} помогли{помочь=V} нам{мы=NI} без{без=PR} вас{вы=NI} совсем{совсем=ADV} бы{бы=PART} нечем{нечего=NI} похоронить{похоронить=V} И{и=S} губы{губа=S} и{и=S} подбородок{подбородок=S} ее{она=NI} вдруг{вдруг=ADV} запрыгали{запрыгать=V} но{но=INTJ} она{она=NI} скрепилась{скрепиться=V} и{и=S} удержалась{удержаться=V} поскорей{скорее=A} опять{опять=ADV} опустив{опустить=GRND} глаза{глаз=S} в{в=S} землю{земля=S}\n",
      "Между{между=PR} разговором{разговор=S} Раскольников{раскольников=S} пристально{пристален=A} ее{она=NI} разглядывал{разглядывать=V} Это{этот=A} было{быть=V} худенькое{худенький=A} совсем{совсем=ADV} худенькое{худенький=A} и{и=S} бледное{бледный=A} личико{личико=S} довольно{доволен=A} неправильное{неправильный=A} какоето{какоето=??} востренькое{востренький=A} с{с=PART} востреньким{востренький=A} маленьким{маленький=A} носом{нос=S} и{и=S} подбородком{подбородок=S} Ее{она=NI} даже{даже=PART} нельзя{нельзя=?} было{быть=V} назвать{назвать=V} и{и=S} хорошенькою{хорошенький=A} но{но=INTJ} зато{зато=S} голубые{голубой=A} глаза{глаз=S} ее{она=NI} были{быть=V} такие{такой=A} ясные{ясный=A} и{и=S} когда{когда=ADV} оживлялись{оживляться=V} они{они=NI} выражение{выражение=S} лица{лицо=S} ее{она=NI} становилось{становиться=V} такое{такой=A} доброе{добрый=A} и{и=S} простодушное{простодушный=A} что{что=PART} невольно{неволен=A} привлекало{привлекать=V} к{к=S} ней{она=NI} В{в=S} лице{лицо=S} ее{она=NI} да{да=INTJ} и{и=S} во{в=PR} всей{всеять=V} ее{она=NI} фигуре{фигура=S} была{быть=V} сверх{сверх=PR} того{тот=A} одна{один=A} особенная{особенный=A} характерная{характерный=A} черта{черта=S} несмотря{несмотря=PR} на{на=PART} свои{свой=A} восемнадцать{восемнадцать=NUM} лет{лет=S} она{она=NI} казалась{казаться=V} почти{почти=PART} еще{еще=ADV} девочкой{девочка=S} гораздо{гораздо=ADV} моложе{моложе=A} своих{свой=A} лет{лет=S} совсем{совсем=ADV} почти{почти=PART} ребенком{ребенок=S} и{и=S} это{этот=A} иногда{иногда=ADV} даже{даже=PART} смешно{смешон=A} проявлялось{проявляться=V} в{в=S} некоторых{некоторый=A} ее{она=NI} движениях{движение=S}\n",
      "Но{но=INTJ} неужели{неужели=INTJ} Катерина{катерина=S} Ивановна{ивановна=S} могла{мочь=V} обойтись{обойтись=V} такими{такой=A} малыми{малый=A} средствами{средство=S} даже{даже=PART} еще{еще=ADV} закуску{закуска=S} намерена{намеренный=PRTS} спросил{спросить=V} Раскольников{раскольников=S} настойчиво{настойчив=A} продолжая{продолжать=GRND} разговор{разговор=S}\n",
      "Гроб{гроб=S} ведь{ведь=CONJ} простой{простоять=V} будетс{будетс=??} и{и=S} всё{все=PART} будет{быть=V} просто{прост=A} так{так=ADV} что{что=PART} недорого{недорог=A} мы{мы=NI} давеча{давеча=ADV} с{с=PART} Катериной{катерина=S} Ивановной{ивановна=S} всё{все=PART} рассчитали{рассчитать=V} так{так=ADV} что{что=PART} и{и=S} останется{остаться=V} чтобы{чтобы=PART} помянуть{помянуть=V} а{а=S} Катерине{катерина=S} Ивановне{ивановна=S} очень{очень=ADV} хочется{хотеться=V} чтобы{чтобы=PART} так{так=ADV} было{быть=V} Ведь{ведь=CONJ} нельзя{нельзя=?} жес{жес=??} ей{она=NI} утешение{утешение=S} она{она=NI} такая{такой=A} ведь{ведь=CONJ} вы{вы=NI} знаете{знаете=CONJ}\n",
      "Понимаю{понимать=V} понимаю{понимать=V} конечно{конечен=A} Что{что=PART} это{этот=A} вы{вы=NI} мою{мыть=V} комнату{комната=S} разглядываете{разглядывать=V} Вот{вот=PART} маменька{маменька=S} говорит{говорить=V} тоже{тоже=PART} что{что=PART} на{на=PART} гроб{гроб=S} похожа{похож=A}\n",
      "Вы{вы=NI} нам{мы=NI} всё{все=PART} вчера{вчера=ADV} отдали{отдать=V} проговорила{проговорить=V} вдруг{вдруг=ADV} в{в=S} ответ{ответ=S} Сонечка{сонечка=S} какимто{какимто=??} сильным{сильный=A} и{и=S} скорым{скорый=A} шепотом{шепотом=ADV} вдруг{вдруг=ADV} опять{опять=ADV} сильно{силен=A} потупившись{потупиться=GRND} Губы{губа=S} и{и=S} подбородок{подбородок=S} ее{она=NI} опять{опять=ADV} запрыгали{запрыгать=V} Она{она=NI} давно{давно=ADV} уже{уже=PART} поражена{пораженный=PRTS} была{быть=V} бедною{бедный=A} обстановкой{обстановка=S} Раскольникова{раскольникова=S} и{и=S} теперь{теперь=ADV} слова{слово=S} эти{этот=A} вдруг{вдруг=ADV} вырвались{вырваться=V} сами{сами=S} собой{себя=NI} Последовало{последовать=V} молчание{молчание=S} Глаза{глаз=S} Дунечки{дунечки=??} както{както=??} прояснели{прояснеть=V} а{а=S} Пульхерия{пульхерия=S} Александровна{александровна=S} даже{даже=PART} приветливо{приветлив=A} посмотрела{посмотреть=V} на{на=PART} Соню{соня=S}\n",
      "Родя{родя=S} сказала{сказать=V} она{она=NI} вставая{вставать=GRND} мы{мы=NI} разумеется{разумеется=CONJ} вместе{вместе=ADV} обедаем{обедать=V} Дунечка{дунечка=??} пойдем{пойти=V} А{а=S} ты{ты=NI} бы{бы=PART} Родя{родя=S} пошел{пойти=V} погулял{погулять=V} немного{немного=NUM} а{а=S} потом{потом=ADV} отдохнул{отдохнуть=V} полежал{полежать=V} а{а=S} там{там=ADV} и{и=S} приходи{приходить=V} скорее{скорее=CONJ} А{а=S} то{тот=A} мы{мы=NI} тебя{ты=NI} утомили{утомить=V} боюсь{бояться=V} я{я=S}\n",
      "Да{да=INTJ} да{да=INTJ} приду{прийти=V} отвечал{отвечать=V} он{он=NI} вставая{вставать=GRND} и{и=S} заторопившись{заторопиться=GRND} У{у=S} меня{я=NI} впрочем{впрочем=CONJ} дело{деть=V}\n",
      "Да{да=INTJ} неужели{неужели=INTJ} ж{ж=S} вы{вы=NI} будете{быть=V} и{и=S} обедать{обедать=V} розно{розен=A} закричал{закричать=V} Разумихин{разумихин=??} с{с=PART} удивлением{удивление=S} смотря{смотреть=GRND} на{на=PART} Раскольникова{раскольникова=S} что{что=PART} ты{ты=NI} это{этот=A}\n",
      "\n",
      "Statistics:\n",
      "Total tokens processed: 193874\n",
      "Tokens not found in dictionary: 7430 (3.83%)\n"
     ]
    }
   ],
   "source": [
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in processed_lines:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(\"Processed text:\")\n",
    "for line in processed_lines[2000:2010]:\n",
    "    print(line)\n",
    "\n",
    "# Печатаем статистику\n",
    "print(\"\\nStatistics:\")\n",
    "print(f\"Total tokens processed: {total_tokens}\")\n",
    "print(f\"Tokens not found in dictionary: {not_found_tokens} ({not_found_tokens/total_tokens:.2%})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
