{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95e16e91",
   "metadata": {},
   "source": [
    "# Лабораторная работа 1\n",
    "Выполнил студент группы 0385 Иванов Серафим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc3ffb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "import bz2\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import shutil\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6e4fee",
   "metadata": {},
   "source": [
    "## Скачивание нужных файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5851543e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файл \"opencorpora_dict.xml.bz2\" успешно скачан в \"downloads\\opencorpora_dict.xml.bz2\"\n",
      "Файл \"сrime_and_punishment.txt\" успешно скачан в \"downloads\\сrime_and_punishment.txt\"\n",
      "Файл \"war_and_peace.txt\" успешно скачан в \"downloads\\war_and_peace.txt\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Папка для сохранения файлов\n",
    "output_dir = 'downloads'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Ссылки на файлы\n",
    "urls = {\n",
    "    'opencorpora_dict.xml.bz2': 'https://opencorpora.org/files/export/dict/dict.opcorpora.xml.bz2',\n",
    "    'сrime_and_punishment.txt': 'https://bookex.info/uploads/public_files/2023-02/prestuplenie-i-nakazanie_-fedor-dostoevskij.txt',\n",
    "    'war_and_peace.txt': 'https://gist.github.com/Semionn/bdcb66640cc070450817686f6c818897/raw/f9e8c888a771dd96f54562a9b050acd1138cc7a9/war_and_peace.ru.txt'\n",
    "}\n",
    "\n",
    "# Скачивание файлов\n",
    "for filename, url in urls.items():\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    urllib.request.urlretrieve(url, output_path)\n",
    "    print(f'Файл \"{filename}\" успешно скачан в \"{output_path}\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6fca08b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file \"opencorpora_dict.xml.bz2\" mod=vet into \"opencorpora_dict.xml.bz2\"\n",
      "file \"war_and_peace.txt\" mod=vet into \"war_and_peace.txt\"\n",
      "file \"сrime_and_punishment.txt\" mod=vet into \"сrime_and_punishment.txt\"\n"
     ]
    }
   ],
   "source": [
    "# Перемещаем все файлы из downloads в родительскую папку\n",
    "parent_dir = os.path.dirname(output_dir)\n",
    "\n",
    "for filename in os.listdir(output_dir):\n",
    "    src_path = os.path.join(output_dir, filename)\n",
    "    dst_path = os.path.join(parent_dir, filename)\n",
    "    if os.path.isfile(src_path):\n",
    "        shutil.move(src_path, dst_path)\n",
    "        print(f'file \"{filename}\" mod=vet into \"{dst_path}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e571eaf9",
   "metadata": {},
   "source": [
    "## Обработка словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cabd771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BZ2_PATH = \"dict.opcorpora.xml.bz2\"   # сжатый словарь\n",
    "XML_PATH = \"dict.opcorpora.xml\"       # распакованный xml\n",
    "PICKLE_PATH = \"lemma_dict.pkl\"        # куда сохраняем готовый словарь\n",
    "\n",
    "if not os.path.exists(XML_PATH) and os.path.exists(BZ2_PATH):\n",
    "    print(\"Unpacking dict...\")\n",
    "    with bz2.open(BZ2_PATH, \"rb\") as f_in, open(XML_PATH, \"wb\") as f_out:\n",
    "        f_out.write(f_in.read())\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9435b4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAMM_TO_SIMPLE = {\n",
    "    \"NOUN\": \"S\", \"S\": \"S\", \"NPRO\": \"NI\",\n",
    "    \"ADJF\": \"A\", \"ADJS\": \"A\", \"COMP\": \"A\",\n",
    "    \"VERB\": \"V\", \"INFN\": \"V\", \"V\": \"V\",\n",
    "    \"PRTF\": \"V\", \"PRTS\": \"V\", \"GRND\": \"V\",\n",
    "    \"ADVB\": \"ADV\", \"ADV\": \"ADV\", \"NUMR\": \"NUM\", \"NUM\": \"NUM\",\n",
    "    \"PREP\": \"PR\", \"PR\": \"PR\", \"CONJ\": \"CONJ\", \"PRCL\": \"PART\", \"INTJ\": \"INTJ\",\n",
    "}\n",
    "POS_GRAMMEMES = set(GRAMM_TO_SIMPLE.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b9723e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(word: str) -> str:\n",
    "    \"\"\"Lower case + ё->е\"\"\"\n",
    "    return word.lower().replace(\"ё\", \"е\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "05849193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting <lemma>...\n",
      "Total: 391,845\n"
     ]
    }
   ],
   "source": [
    "print(\"Counting <lemma>...\")\n",
    "lemma_count = 0\n",
    "for _, elem in ET.iterparse(XML_PATH, events=(\"end\",)):\n",
    "    if elem.tag == \"lemma\":\n",
    "        lemma_count += 1\n",
    "    elem.clear()\n",
    "print(f\"Total: {lemma_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2bc0f949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating lemma dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391845/391845 [00:44<00:00, 8813.50lemma/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dict is ready, total keys: 3060604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lemma_dict = dict()\n",
    "\n",
    "print(\"Creating lemma dict...\")\n",
    "it = ET.iterparse(XML_PATH, events=(\"end\",))\n",
    "with tqdm(total=lemma_count, unit=\"lemma\") as pbar:\n",
    "    for event, elem in it:\n",
    "        if elem.tag == \"lemma\":\n",
    "            l_elem = elem.find(\"l\")\n",
    "            if l_elem is not None:\n",
    "                lemma_text = l_elem.get(\"t\") or \"\"\n",
    "                pos_simple = None\n",
    "                for g in l_elem.findall(\"g\"):\n",
    "                    v = g.get(\"v\")\n",
    "                    if v in POS_GRAMMEMES:\n",
    "                        pos_simple = GRAMM_TO_SIMPLE[v]\n",
    "                        break\n",
    "                if pos_simple is None:\n",
    "                    pos_simple = \"?\"\n",
    "\n",
    "                # обавляем все формы <f> плюс лемму ---\n",
    "                forms = [lemma_text] + [f.get(\"t\") for f in elem.findall(\"f\") if f.get(\"t\")]\n",
    "                for form in forms:\n",
    "                    norm = normalize(form)\n",
    "                    lemma_norm = normalize(lemma_text)\n",
    "                    lemma_dict[norm] = (lemma_norm, pos_simple)\n",
    "\n",
    "            elem.clear()\n",
    "            pbar.update(1)\n",
    "\n",
    "print(\"The dict is ready, total keys:\", len(lemma_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "db4eb35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to lemma_dict.pkl\n"
     ]
    }
   ],
   "source": [
    "with open(PICKLE_PATH, \"wb\") as f:\n",
    "    pickle.dump(lemma_dict, f, protocol=4)\n",
    "print(\"Saved to\", PICKLE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b76b7cb",
   "metadata": {},
   "source": [
    "## Обработка текста"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd19de85",
   "metadata": {},
   "source": [
    "Загружаем готовый словарь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22701f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict loaded, keys: 3060604\n"
     ]
    }
   ],
   "source": [
    "PICKLE_PATH = \"lemma_dict.pkl\"\n",
    "with open(PICKLE_PATH, \"rb\") as f:\n",
    "    lemma_dict = pickle.load(f)\n",
    "print(\"Dict loaded, keys:\", len(lemma_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb95f14",
   "metadata": {},
   "source": [
    "Функции для лемматизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26cb4277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# глобальные счётчики\n",
    "total_tokens = 0\n",
    "not_found_tokens = 0\n",
    "\n",
    "def normalize(word: str) -> str:\n",
    "    \"\"\"Normalize a token: convert to lowercase and replace 'ё' with 'е'\"\"\"\n",
    "    return word.lower().replace(\"ё\", \"е\")\n",
    "\n",
    "def lemmatize_token(token: str):\n",
    "    \"\"\"\n",
    "    Returns the token in the format: token{lemma=POS}.\n",
    "    Updates global counters:\n",
    "      - total_tokens\n",
    "      - not_found_tokens\n",
    "    \"\"\"\n",
    "    global total_tokens, not_found_tokens\n",
    "    total_tokens += 1\n",
    "\n",
    "    norm = normalize(token)\n",
    "    if norm in lemma_dict:\n",
    "        lemma, pos = lemma_dict[norm]\n",
    "        return f\"{token}{{{lemma}={pos}}}\"\n",
    "    else:\n",
    "        not_found_tokens += 1\n",
    "        return f\"{token}{{{norm}=??}}\"\n",
    "\n",
    "def process_sentence(sentence: str):\n",
    "    \"\"\"\n",
    "    1. Remove all punctuation\n",
    "    2. Split the sentence into tokens\n",
    "    3. Lemmatize each token (updates counters)\n",
    "    4. Join tokens back into a string\n",
    "    \"\"\"\n",
    "    clean = re.sub(r\"[^\\w\\s]\", \"\", sentence, flags=re.UNICODE)\n",
    "    tokens = clean.split()\n",
    "    return \" \".join(lemmatize_token(tok) for tok in tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ca653f",
   "metadata": {},
   "source": [
    "Использование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bd7a9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "Стала стабильнее экономическая и политическая обстановка, предприятия вывели из тени зарплаты сотрудников.\n",
      "Все Гришины одноклассники уже побывали за границей, он был чуть ли не единственным, кого не вывозили никуда дальше Красной Пахры.\n",
      "\n",
      "\n",
      "Output:\n",
      "Стала{стал=V} стабильнее{стабильнее=A} экономическая{экономический=A} и{и=S} политическая{политический=A} обстановка{обстановка=S} предприятия{предприятие=S} вывели{вывел=V} из{иза=S} тени{тень=S} зарплаты{зарплата=S} сотрудников{сотрудник=S}\n",
      "Все{все=PART} Гришины{гришин=A} одноклассники{одноклассник=S} уже{уже=PART} побывали{побывал=V} за{за=PR} границей{граница=S} он{он=NI} был{есть=V} чуть{чуть=CONJ} ли{ли=S} не{не=PART} единственным{единственный=A} кого{кто=NI} не{не=PART} вывозили{вывозил=V} никуда{никуда=ADV} дальше{дальше=A} Красной{красный=A} Пахры{пахра=S}\n",
      "\n",
      "Statistics:\n",
      "Total tokens processed: 32\n",
      "Tokens not found in dictionary: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# Сброс глобальных счётчиков перед обработкой\n",
    "total_tokens = 0\n",
    "not_found_tokens = 0\n",
    "\n",
    "input_text = \"\"\"Стала стабильнее экономическая и политическая обстановка, предприятия вывели из тени зарплаты сотрудников.\n",
    "Все Гришины одноклассники уже побывали за границей, он был чуть ли не единственным, кого не вывозили никуда дальше Красной Пахры.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Input:\")\n",
    "print(input_text)\n",
    "\n",
    "print(\"\\nOutput:\")\n",
    "for line in input_text.strip().split(\"\\n\"):\n",
    "    print(process_sentence(line))\n",
    "\n",
    "# Печатаем статистику\n",
    "print(\"\\nStatistics:\")\n",
    "print(f\"Total tokens processed: {total_tokens}\")\n",
    "print(f\"Tokens not found in dictionary: {not_found_tokens} ({not_found_tokens/total_tokens:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc5c71d",
   "metadata": {},
   "source": [
    "## Обработка большого текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4aa0a4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"сrime_and_punishment.txt\"   # путь к файлу с текстом\n",
    "OUTPUT_FILE = \"сrime_and_punishment_out.txt\" # куда сохранить результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cb04bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кодировки:\n",
    "# Война и мир UTF-8\n",
    "# Преступление и наказание windows 1251\n",
    "\n",
    "# Сброс глобальных счётчиков перед обработкой\n",
    "total_tokens = 0\n",
    "not_found_tokens = 0\n",
    "\n",
    "with open(INPUT_FILE, \"r\", encoding=\"UTF-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "processed_lines = []\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if line:  # skip empty lines\n",
    "        processed_lines.append(process_sentence(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ea3c916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed text:\n",
      "Она{она=NI} поминки{поминки=S} устраивает{устраиваю=V}\n",
      "Дас{дас=??} закуску{закуска=S} она{она=NI} вас{вы=NI} очень{очень=ADV} велела{велю=V} благодарить{благодарить=V} что{что=PART} вы{вы=NI} вчера{вчера=ADV} помогли{помог=V} нам{мы=NI} без{без=PR} вас{вы=NI} совсем{совсем=ADV} бы{бы=PART} нечем{нечего=NI} похоронить{похоронить=V} И{и=S} губы{губа=S} и{и=S} подбородок{подбородок=S} ее{она=NI} вдруг{вдруг=ADV} запрыгали{запрыгал=V} но{но=INTJ} она{она=NI} скрепилась{скрепился=V} и{и=S} удержалась{удержался=V} поскорей{скорее=A} опять{опять=ADV} опустив{опустив=V} глаза{глаз=S} в{в=S} землю{земля=S}\n",
      "Между{между=PR} разговором{разговор=S} Раскольников{раскольников=S} пристально{пристален=A} ее{она=NI} разглядывал{разглядываю=V} Это{этот=A} было{есть=V} худенькое{худенький=A} совсем{совсем=ADV} худенькое{худенький=A} и{и=S} бледное{бледный=A} личико{личико=S} довольно{доволен=A} неправильное{неправильный=A} какоето{какоето=??} востренькое{востренький=A} с{с=PART} востреньким{востренький=A} маленьким{маленький=A} носом{нос=S} и{и=S} подбородком{подбородок=S} Ее{она=NI} даже{даже=PART} нельзя{нельзя=?} было{есть=V} назвать{назвать=V} и{и=S} хорошенькою{хорошенький=A} но{но=INTJ} зато{зато=S} голубые{голубой=A} глаза{глаз=S} ее{она=NI} были{есть=V} такие{такой=A} ясные{ясный=A} и{и=S} когда{когда=ADV} оживлялись{оживляюсь=V} они{они=NI} выражение{выражение=S} лица{лицо=S} ее{она=NI} становилось{становлюсь=V} такое{такой=A} доброе{добрый=A} и{и=S} простодушное{простодушный=A} что{что=PART} невольно{неволен=A} привлекало{привлекаю=V} к{к=S} ней{она=NI} В{в=S} лице{лицо=S} ее{она=NI} да{да=INTJ} и{и=S} во{в=PR} всей{всеял=V} ее{она=NI} фигуре{фигура=S} была{есть=V} сверх{сверх=PR} того{тот=A} одна{один=A} особенная{особенный=A} характерная{характерный=A} черта{черта=S} несмотря{несмотря=PR} на{на=PART} свои{свой=A} восемнадцать{восемнадцать=NUM} лет{лет=S} она{она=NI} казалась{кажусь=V} почти{почти=PART} еще{еще=ADV} девочкой{девочка=S} гораздо{гораздо=ADV} моложе{моложе=A} своих{свой=A} лет{лет=S} совсем{совсем=ADV} почти{почти=PART} ребенком{ребенок=S} и{и=S} это{этот=A} иногда{иногда=ADV} даже{даже=PART} смешно{смешон=A} проявлялось{проявляюсь=V} в{в=S} некоторых{некоторый=A} ее{она=NI} движениях{движение=S}\n",
      "Но{но=INTJ} неужели{неужели=INTJ} Катерина{катерина=S} Ивановна{ивановна=S} могла{могу=V} обойтись{обойтись=V} такими{такой=A} малыми{малый=A} средствами{средство=S} даже{даже=PART} еще{еще=ADV} закуску{закуска=S} намерена{намерен=V} спросил{спросил=V} Раскольников{раскольников=S} настойчиво{настойчив=A} продолжая{продолжая=V} разговор{разговор=S}\n",
      "Гроб{гроб=S} ведь{ведь=CONJ} простой{простоял=V} будетс{будетс=??} и{и=S} всё{все=PART} будет{есть=V} просто{прост=A} так{так=ADV} что{что=PART} недорого{недорог=A} мы{мы=NI} давеча{давеча=ADV} с{с=PART} Катериной{катерина=S} Ивановной{ивановна=S} всё{все=PART} рассчитали{рассчитал=V} так{так=ADV} что{что=PART} и{и=S} останется{остался=V} чтобы{чтобы=PART} помянуть{помянуть=V} а{а=S} Катерине{катерина=S} Ивановне{ивановна=S} очень{очень=ADV} хочется{хотелось=V} чтобы{чтобы=PART} так{так=ADV} было{есть=V} Ведь{ведь=CONJ} нельзя{нельзя=?} жес{жес=??} ей{она=NI} утешение{утешение=S} она{она=NI} такая{такой=A} ведь{ведь=CONJ} вы{вы=NI} знаете{знаете=CONJ}\n",
      "Понимаю{понимаю=V} понимаю{понимаю=V} конечно{конечен=A} Что{что=PART} это{этот=A} вы{вы=NI} мою{мою=V} комнату{комната=S} разглядываете{разглядываю=V} Вот{вот=PART} маменька{маменька=S} говорит{говорю=V} тоже{тоже=PART} что{что=PART} на{на=PART} гроб{гроб=S} похожа{похож=A}\n",
      "Вы{вы=NI} нам{мы=NI} всё{все=PART} вчера{вчера=ADV} отдали{отдал=V} проговорила{проговорил=V} вдруг{вдруг=ADV} в{в=S} ответ{ответ=S} Сонечка{сонечка=S} какимто{какимто=??} сильным{сильный=A} и{и=S} скорым{скорый=A} шепотом{шепотом=ADV} вдруг{вдруг=ADV} опять{опять=ADV} сильно{силен=A} потупившись{потупившись=V} Губы{губа=S} и{и=S} подбородок{подбородок=S} ее{она=NI} опять{опять=ADV} запрыгали{запрыгал=V} Она{она=NI} давно{давно=ADV} уже{уже=PART} поражена{поражен=V} была{есть=V} бедною{бедный=A} обстановкой{обстановка=S} Раскольникова{раскольникова=S} и{и=S} теперь{теперь=ADV} слова{слово=S} эти{этот=A} вдруг{вдруг=ADV} вырвались{вырвался=V} сами{сами=S} собой{себя=NI} Последовало{последовал=V} молчание{молчание=S} Глаза{глаз=S} Дунечки{дунечки=??} както{както=??} прояснели{прояснел=V} а{а=S} Пульхерия{пульхерия=S} Александровна{александровна=S} даже{даже=PART} приветливо{приветлив=A} посмотрела{посмотрел=V} на{на=PART} Соню{соня=S}\n",
      "Родя{родя=S} сказала{сказал=V} она{она=NI} вставая{вставая=V} мы{мы=NI} разумеется{разумеется=CONJ} вместе{вместе=ADV} обедаем{обедаю=V} Дунечка{дунечка=??} пойдем{пошел=V} А{а=S} ты{ты=NI} бы{бы=PART} Родя{родя=S} пошел{пошел=V} погулял{погулял=V} немного{немного=NUM} а{а=S} потом{потом=ADV} отдохнул{отдохнул=V} полежал{полежал=V} а{а=S} там{там=ADV} и{и=S} приходи{прихожу=V} скорее{скорее=CONJ} А{а=S} то{тот=A} мы{мы=NI} тебя{ты=NI} утомили{утомил=V} боюсь{боюсь=V} я{я=S}\n",
      "Да{да=INTJ} да{да=INTJ} приду{пришел=V} отвечал{отвечаю=V} он{он=NI} вставая{вставая=V} и{и=S} заторопившись{заторопившись=V} У{у=S} меня{я=NI} впрочем{впрочем=CONJ} дело{дел=V}\n",
      "Да{да=INTJ} неужели{неужели=INTJ} ж{ж=S} вы{вы=NI} будете{есть=V} и{и=S} обедать{обедать=V} розно{розен=A} закричал{закричал=V} Разумихин{разумихин=??} с{с=PART} удивлением{удивление=S} смотря{смотря=V} на{на=PART} Раскольникова{раскольникова=S} что{что=PART} ты{ты=NI} это{этот=A}\n",
      "\n",
      "Statistics:\n",
      "Total tokens processed: 193874\n",
      "Tokens not found in dictionary: 7430 (3.83%)\n"
     ]
    }
   ],
   "source": [
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in processed_lines:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(\"Processed text:\")\n",
    "for line in processed_lines[2000:2010]:\n",
    "    print(line)\n",
    "\n",
    "# Печатаем статистику\n",
    "print(\"\\nStatistics:\")\n",
    "print(f\"Total tokens processed: {total_tokens}\")\n",
    "print(f\"Tokens not found in dictionary: {not_found_tokens} ({not_found_tokens/total_tokens:.2%})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
