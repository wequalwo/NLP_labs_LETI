{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95e16e91",
   "metadata": {},
   "source": [
    "# Лабораторная работа 1\n",
    "Выполнил студент группы 0385 Иванов Серафим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cc3ffb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "import bz2\n",
    "import os\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6e4fee",
   "metadata": {},
   "source": [
    "## Скачивание нужных файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5851543e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файл \"opencorpora_dict.xml.bz2\" успешно скачан в \"downloads\\opencorpora_dict.xml.bz2\"\n",
      "Файл \"сrime_and_punishment.txt\" успешно скачан в \"downloads\\сrime_and_punishment.txt\"\n",
      "Файл \"war_and_peace.txt\" успешно скачан в \"downloads\\war_and_peace.txt\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Папка для сохранения файлов\n",
    "output_dir = 'downloads'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Ссылки на файлы\n",
    "urls = {\n",
    "    'opencorpora_dict.xml.bz2': 'https://opencorpora.org/files/export/dict/dict.opcorpora.xml.bz2',\n",
    "    'сrime_and_punishment.txt': 'https://royallib.com/get/txt/dostoevskiy_fedor/prestuplenie_i_nakazanie.zip',\n",
    "    'war_and_peace.txt': 'https://gist.github.com/Semionn/bdcb66640cc070450817686f6c818897/raw/f9e8c888a771dd96f54562a9b050acd1138cc7a9/war_and_peace.ru.txt'\n",
    "}\n",
    "\n",
    "# Скачивание файлов\n",
    "for filename, url in urls.items():\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    urllib.request.urlretrieve(url, output_path)\n",
    "    print(f'Файл \"{filename}\" успешно скачан в \"{output_path}\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e571eaf9",
   "metadata": {},
   "source": [
    "## Обработка словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabd771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BZ2_PATH = \"dict.opcorpora.xml.bz2\"   # сжатый словарь\n",
    "XML_PATH = \"dict.opcorpora.xml\"       # распакованный xml\n",
    "PICKLE_PATH = \"lemma_dict.pkl\"        # куда сохраняем готовый словарь\n",
    "\n",
    "if not os.path.exists(XML_PATH) and os.path.exists(BZ2_PATH):\n",
    "    print(\"Unpacking dict...\")\n",
    "    with bz2.open(BZ2_PATH, \"rb\") as f_in, open(XML_PATH, \"wb\") as f_out:\n",
    "        f_out.write(f_in.read())\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9435b4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAMM_TO_SIMPLE = {\n",
    "    \"NOUN\": \"S\", \"S\": \"S\", \"NPRO\": \"NI\",\n",
    "    \"ADJF\": \"A\", \"ADJS\": \"A\", \"COMP\": \"A\",\n",
    "    \"VERB\": \"V\", \"INFN\": \"V\", \"V\": \"V\",\n",
    "    \"PRTF\": \"V\", \"PRTS\": \"V\", \"GRND\": \"V\",\n",
    "    \"ADVB\": \"ADV\", \"ADV\": \"ADV\", \"NUMR\": \"NUM\", \"NUM\": \"NUM\",\n",
    "    \"PREP\": \"PR\", \"PR\": \"PR\", \"CONJ\": \"CONJ\", \"PRCL\": \"PART\", \"INTJ\": \"INTJ\",\n",
    "}\n",
    "POS_GRAMMEMES = set(GRAMM_TO_SIMPLE.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9723e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(word: str) -> str:\n",
    "    \"\"\"Lower case + ё->е\"\"\"\n",
    "    return word.lower().replace(\"ё\", \"е\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05849193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Считаю количество <lemma>...\n",
      "Всего лемм: 391,845\n"
     ]
    }
   ],
   "source": [
    "print(\"Counting <lemma>...\")\n",
    "lemma_count = 0\n",
    "for _, elem in ET.iterparse(XML_PATH, events=(\"end\",)):\n",
    "    if elem.tag == \"lemma\":\n",
    "        lemma_count += 1\n",
    "    elem.clear()\n",
    "print(f\"Total: {lemma_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2bc0f949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating lemma dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391845/391845 [00:55<00:00, 7121.52lemma/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dict is ready, total keys: 3060604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lemma_dict = dict()\n",
    "\n",
    "print(\"Creating lemma dict...\")\n",
    "it = ET.iterparse(XML_PATH, events=(\"end\",))\n",
    "with tqdm(total=lemma_count, unit=\"lemma\") as pbar:\n",
    "    for event, elem in it:\n",
    "        if elem.tag == \"lemma\":\n",
    "            l_elem = elem.find(\"l\")\n",
    "            if l_elem is not None:\n",
    "                lemma_text = l_elem.get(\"t\") or \"\"\n",
    "                pos_simple = None\n",
    "                for g in l_elem.findall(\"g\"):\n",
    "                    v = g.get(\"v\")\n",
    "                    if v in POS_GRAMMEMES:\n",
    "                        pos_simple = GRAMM_TO_SIMPLE[v]\n",
    "                        break\n",
    "                if pos_simple is None:\n",
    "                    pos_simple = \"?\"\n",
    "\n",
    "                # обавляем все формы <f> плюс лемму ---\n",
    "                forms = [lemma_text] + [f.get(\"t\") for f in elem.findall(\"f\") if f.get(\"t\")]\n",
    "                for form in forms:\n",
    "                    norm = normalize(form)\n",
    "                    lemma_norm = normalize(lemma_text)\n",
    "                    lemma_dict[norm] = (lemma_norm, pos_simple)\n",
    "\n",
    "            elem.clear()\n",
    "            pbar.update(1)\n",
    "\n",
    "print(\"The dict is ready, total keys:\", len(lemma_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4eb35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сохранено в lemma_dict.pkl\n"
     ]
    }
   ],
   "source": [
    "with open(PICKLE_PATH, \"wb\") as f:\n",
    "    pickle.dump(lemma_dict, f, protocol=4)\n",
    "print(\"Saved to\", PICKLE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b76b7cb",
   "metadata": {},
   "source": [
    "## Обработка текста"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd19de85",
   "metadata": {},
   "source": [
    "Загружаем готовый словарь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22701f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict loaded, keys: 3060604\n"
     ]
    }
   ],
   "source": [
    "PICKLE_PATH = \"lemma_dict.pkl\"\n",
    "with open(PICKLE_PATH, \"rb\") as f:\n",
    "    lemma_dict = pickle.load(f)\n",
    "print(\"Dict loaded, keys:\", len(lemma_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb95f14",
   "metadata": {},
   "source": [
    "Функции для лемматизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "26cb4277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# глобальные счётчики\n",
    "total_tokens = 0\n",
    "not_found_tokens = 0\n",
    "\n",
    "def normalize(word: str) -> str:\n",
    "    \"\"\"Normalize a token: convert to lowercase and replace 'ё' with 'е'\"\"\"\n",
    "    return word.lower().replace(\"ё\", \"е\")\n",
    "\n",
    "def lemmatize_token(token: str):\n",
    "    \"\"\"\n",
    "    Returns the token in the format: token{lemma=POS}.\n",
    "    Updates global counters:\n",
    "      - total_tokens\n",
    "      - not_found_tokens\n",
    "    \"\"\"\n",
    "    global total_tokens, not_found_tokens\n",
    "    total_tokens += 1\n",
    "\n",
    "    norm = normalize(token)\n",
    "    if norm in lemma_dict:\n",
    "        lemma, pos = lemma_dict[norm]\n",
    "        return f\"{token}{{{lemma}={pos}}}\"\n",
    "    else:\n",
    "        not_found_tokens += 1\n",
    "        return f\"{token}{{{norm}=??}}\"\n",
    "\n",
    "def process_sentence(sentence: str):\n",
    "    \"\"\"\n",
    "    1. Remove all punctuation\n",
    "    2. Split the sentence into tokens\n",
    "    3. Lemmatize each token (updates counters)\n",
    "    4. Join tokens back into a string\n",
    "    \"\"\"\n",
    "    clean = re.sub(r\"[^\\w\\s]\", \"\", sentence, flags=re.UNICODE)\n",
    "    tokens = clean.split()\n",
    "    return \" \".join(lemmatize_token(tok) for tok in tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ca653f",
   "metadata": {},
   "source": [
    "Использование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9bd7a9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "Стала стабильнее экономическая и политическая обстановка, предприятия вывели из тени зарплаты сотрудников.\n",
      "Все Гришины одноклассники уже побывали за границей, он был чуть ли не единственным, кого не вывозили никуда дальше Красной Пахры.\n",
      "\n",
      "\n",
      "Output:\n",
      "Стала{стал=V} стабильнее{стабильнее=A} экономическая{экономический=A} и{и=S} политическая{политический=A} обстановка{обстановка=S} предприятия{предприятие=S} вывели{вывел=V} из{иза=S} тени{тень=S} зарплаты{зарплата=S} сотрудников{сотрудник=S}\n",
      "Все{все=PART} Гришины{гришин=A} одноклассники{одноклассник=S} уже{уже=PART} побывали{побывал=V} за{за=PR} границей{граница=S} он{он=NI} был{есть=V} чуть{чуть=CONJ} ли{ли=S} не{не=PART} единственным{единственный=A} кого{кто=NI} не{не=PART} вывозили{вывозил=V} никуда{никуда=ADV} дальше{дальше=A} Красной{красный=A} Пахры{пахра=S}\n",
      "\n",
      "Statistics:\n",
      "Total tokens processed: 32\n",
      "Tokens not found in dictionary: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# Сброс глобальных счётчиков перед обработкой\n",
    "total_tokens = 0\n",
    "not_found_tokens = 0\n",
    "\n",
    "input_text = \"\"\"Стала стабильнее экономическая и политическая обстановка, предприятия вывели из тени зарплаты сотрудников.\n",
    "Все Гришины одноклассники уже побывали за границей, он был чуть ли не единственным, кого не вывозили никуда дальше Красной Пахры.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Input:\")\n",
    "print(input_text)\n",
    "\n",
    "print(\"\\nOutput:\")\n",
    "for line in input_text.strip().split(\"\\n\"):\n",
    "    print(process_sentence(line))\n",
    "\n",
    "# Печатаем статистику\n",
    "print(\"\\nStatistics:\")\n",
    "print(f\"Total tokens processed: {total_tokens}\")\n",
    "print(f\"Tokens not found in dictionary: {not_found_tokens} ({not_found_tokens/total_tokens:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc5c71d",
   "metadata": {},
   "source": [
    "## Обработка большого текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa0a4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"war_and_peace.txt\"   # путь к файлу с текстом\n",
    "OUTPUT_FILE = \"war_and_peace_out.txt\" # куда сохранить результат\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4cb04bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кодировки:\n",
    "# Война и мир UTF-8\n",
    "# Преступление и наказание windows 1251\n",
    "\n",
    "# Сброс глобальных счётчиков перед обработкой\n",
    "total_tokens = 0\n",
    "not_found_tokens = 0\n",
    "\n",
    "with open(INPUT_FILE, \"r\", encoding=\"UTF-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "processed_lines = []\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if line:  # skip empty lines\n",
    "        processed_lines.append(process_sentence(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0ea3c916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed text:\n",
      "Солдату{солдат=S} позорно{позорен=A} красть{красть=V} солдат{солдат=S} должен{должен=A} быть{быть=V} честен{честен=A} благороден{благороден=A} и{и=S} храбр{храбр=A} а{а=S} коли{коля=S} у{у=S} своего{свой=A} брата{брат=S} украл{украл=V} так{так=ADV} в{в=S} нем{нем=A} чести{честь=S} нет{нет=?} это{этот=A} мерзавец{мерзавец=S} Еще{еще=ADV} еще{еще=ADV}\n",
      "И{и=S} всё{все=PART} слышались{слышусь=V} гибкие{гибкий=A} удары{удар=S} и{и=S} отчаянный{отчаянный=A} но{но=INTJ} притворный{притворный=A} крик{крик=S}\n",
      "Еще{еще=ADV} еще{еще=ADV} приговаривал{приговариваю=V} майор{майор=S}\n",
      "Молодой{молодой=A} офицер{офицер=S} с{с=PART} выражением{выражение=S} недоумения{недоумение=S} и{и=S} страдания{страдание=S} в{в=S} лице{лицо=S} отошел{отошел=V} от{от=PR} наказываемого{наказываемый=V} оглядываясь{оглядываясь=V} вопросительно{вопросителен=A} на{на=PART} проезжавшего{проезжавший=V} адъютанта{адъютант=S}\n",
      "Князь{князь=S} Андрей{андрей=S} выехав{выехав=V} в{в=S} переднюю{передняя=S} линию{линия=S} поехал{поехал=V} по{по=S} фронту{фронт=S} Цепь{цепь=S} наша{наш=A} и{и=S} неприятельская{неприятельский=A} стояли{стою=V} на{на=PART} левом{левый=A} и{и=S} на{на=PART} правом{правый=A} фланге{фланг=S} далеко{далеко=ADV} друг{друг=S} от{от=PR} друга{друг=S} но{но=INTJ} в{в=S} средине{средина=S} в{в=S} том{тот=A} месте{место=S} где{где=CONJ} утром{утром=ADV} проезжали{проезжаю=V} парламентеры{парламентер=S} цепи{цепь=S} сошлись{сослался=V} так{так=ADV} близко{близко=ADV} что{что=PART} могли{могу=V} видеть{видеть=V} лица{лицо=S} друг{друг=S} друга{друг=S} и{и=S} переговариваться{переговариваться=V} между{между=PR} собой{себя=NI} Кроме{кроме=PR} солдат{солдат=S} занимавших{занимавший=V} цепь{цепь=S} в{в=S} этом{этот=A} месте{место=S} с{с=PART} той{тот=A} и{и=S} с{с=PART} другой{другой=A} стороны{сторона=S} стояло{стою=V} много{много=NUM} любопытных{любопытный=A} которые{который=A} посмеиваясь{посмеиваясь=V} разглядывали{разглядываю=V} странных{странный=A} и{и=S} чуждых{чуждый=A} для{для=PR} них{они=NI} неприятелей{неприятель=S}\n",
      "С{с=PART} раннего{ранний=A} утра{утро=S} несмотря{несмотря=PR} на{на=PART} запрещение{запрещение=S} подходить{подходить=V} к{к=S} цепи{цепь=S} начальники{начальник=S} не{не=PART} могли{могу=V} отбиться{отбиться=V} от{от=PR} любопытных{любопытный=A} Солдаты{солдат=S} стоявшие{стоявший=V} в{в=S} цепи{цепь=S} как{как=ADV} люди{человек=S} показывающие{показывающий=V} что{что=PART} нибудь{нибудь=??} редкое{редкий=A} уж{уж=S} не{не=PART} смотрели{смотрю=V} на{на=PART} французов{француз=S} а{а=S} делали{делаю=V} свои{свой=A} наблюдения{наблюдение=S} над{над=PR} приходящими{приходящий=A} и{и=S} скучая{скучая=V} дожидались{дожидаюсь=V} смены{смена=S} Князь{князь=S} Андрей{андрей=S} остановился{остановился=V} рассматривать{рассматривать=V} французов{француз=S}\n",
      "Глянь{глянул=V} ка{ка=PART} глянь{глянул=V} говорил{говорю=V} один{один=A} солдат{солдат=S} товарищу{товарищ=S} указывая{указывая=V} на{на=PART} русского{русский=A} мушкатера{мушкатера=??} солдата{солдат=S} который{который=A} с{с=PART} офицером{офицер=S} подошел{подошел=V} к{к=S} цепи{цепь=S} и{и=S} что{что=PART} то{тот=A} часто{част=A} и{и=S} горячо{горячо=ADV} говорил{говорю=V} с{с=PART} французским{французский=A} гренадером{гренадер=S} Вишь{вишь=PART} лопочет{лопочу=V} как{как=ADV} ловко{ловко=ADV} Аж{ажа=S} хранцуз{хранцуз=??} то{тот=A} за{за=PR} ним{оно=NI} не{не=PART} поспевает{поспеваю=V} Ну{ну=PART} ка{ка=PART} ты{ты=NI} Сидоров{сидоров=S}\n",
      "Погоди{погодил=V} послушай{послушал=V} Ишь{ишь=PART} ловко{ловко=ADV} отвечал{отвечаю=V} Сидоров{сидоров=S} считавшийся{считавшийся=V} мастером{мастер=S} говорить{говорить=V} по{по=S} французски{французски=??}\n",
      "Солдат{солдат=S} на{на=PART} которого{который=A} указывали{указываю=V} смеявшиеся{смеявшийся=V} был{есть=V} Долохов{долохов=??} Князь{князь=S} Андрей{андрей=S} узнал{узнал=V} его{оно=NI} и{и=S} прислушался{прислушался=V} к{к=S} его{оно=NI} разговору{разговор=S} Долохов{долохов=??} вместе{вместе=ADV} с{с=PART} своим{свой=A} ротным{ротный=A} пришел{пришел=V} в{в=S} цепь{цепь=S} с{с=PART} левого{левый=A} фланга{фланг=S} на{на=PART} котором{который=A} стоял{стою=V} их{они=NI} полк{полк=S}\n",
      "Ну{ну=PART} еще{еще=ADV} еще{еще=ADV} подстрекал{подстрекаю=V} ротный{ротный=A} командир{командир=S} нагибаясь{нагибаясь=V} вперед{вперед=PR} и{и=S} стараясь{стараясь=V} не{не=PART} проронить{проронить=V} ни{ни=PART} одного{один=A} непонятного{непонятный=A} для{для=PR} него{оно=NI} слова{слово=S} Пожалуйста{пожалуйста=INTJ} почаще{чаще=A} Что{что=PART} он{он=NI}\n",
      "\n",
      "Statistics:\n",
      "Total tokens processed: 230335\n",
      "Tokens not found in dictionary: 12434 (5.40%)\n"
     ]
    }
   ],
   "source": [
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in processed_lines:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(\"Processed text:\")\n",
    "for line in processed_lines[2000:2010]:\n",
    "    print(line)\n",
    "\n",
    "# Печатаем статистику\n",
    "print(\"\\nStatistics:\")\n",
    "print(f\"Total tokens processed: {total_tokens}\")\n",
    "print(f\"Tokens not found in dictionary: {not_found_tokens} ({not_found_tokens/total_tokens:.2%})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
