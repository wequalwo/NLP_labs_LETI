{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902c768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca90a732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wequalwo/LETI/1. NLP/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from razdel import sentenize\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import MBartTokenizer, MBartForConditionalGeneration\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38dfa437",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "model_name = \"sarahai/ru-sum\" #\"cointegrated/rut5-base-absum\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acdd0f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(30000, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(30000, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(30000, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=30000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#для \"sarahai/ru-sum\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "model.eval()\n",
    "\n",
    "# для \"cointegrated/rut5-base-absum\"\n",
    "# model_name = \"cointegrated/rut5-base-absum\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c135fab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def abstractive_summary(text, max_symbols=300):\n",
    "#     inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "#     summary_ids = model.generate(\n",
    "#           **inputs,\n",
    "#           max_length=100,\n",
    "#           min_length=60,\n",
    "#           num_beams=30,\n",
    "#           repetition_penalty=3.0,\n",
    "#           length_penalty=1.2,\n",
    "#           no_repeat_ngram_size=10,\n",
    "#           early_stopping=True\n",
    "#       )\n",
    "#     summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "#     return summary[:max_symbols]\n",
    "\n",
    "\n",
    "def improved_abstractive_summary(text, max_symbols=300, device='cuda'):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to('mps')\n",
    "    with torch.no_grad():\n",
    "        summary_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_length=150,          # подогнал под размер 300 на глаз\n",
    "            min_length=60,\n",
    "            num_beams=30,            # взял прямо жирно, но так мне больше нравится\n",
    "            temperature=0.9,         # Для чуть большей вариативности\n",
    "            repetition_penalty=1.5,  # Мягче штрафуем повторения\n",
    "            length_penalty=1.0,      # Нейтральное влияние на длину\n",
    "            no_repeat_ngram_size=3,  \n",
    "            early_stopping=True,\n",
    "            do_sample=True          \n",
    "        )\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary[:max_symbols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40975343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing texts: 100%|██████████| 5/5 [00:37<00:00,  7.48s/it]\n"
     ]
    }
   ],
   "source": [
    "with open(\"ex2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    texts = json.load(f)\n",
    "\n",
    "summaries = []\n",
    "for text in tqdm(texts, desc=\"Summarizing texts\"):\n",
    "    abs_summary = improved_abstractive_summary(text)\n",
    "    summaries.append(abs_summary)\n",
    "\n",
    "with open(\"summaries.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summaries, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec338ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длинна исходного текста: 1054, Длинна реферата: 163, Длинна референса: 194\n",
      "Длинна исходного текста: 1101, Длинна реферата: 172, Длинна референса: 223\n",
      "Длинна исходного текста: 807, Длинна реферата: 181, Длинна референса: 208\n",
      "Длинна исходного текста: 684, Длинна реферата: 178, Длинна референса: 193\n",
      "Длинна исходного текста: 740, Длинна реферата: 182, Длинна референса: 190\n"
     ]
    }
   ],
   "source": [
    "reference_summaries = [\n",
    "    \"Пёс Дик любит смотреть, как я курю. На болоте я нашёл черничную поляну, встал на четвереньки и стал есть ягоды прямо с куста. Дик наблюдал и радовался, а потом сам собрал чернику вокруг избушки.\",\n",
    "    \"Скворца у нас любят больше всех. У поэта жил скворец Петруша, он сочинял стихи и печатал их в журналах. За каждую строчку платили рубль и сорок копеек. Петруша подражал голосам и звукам, его трели напоминали весенние песни.\",\n",
    "    \"Заяц собрал букет полевых цветов и не знает, кому его подарить. Лисе и волку заячий букет не нужен, у русаков цветы над ушами, у беляков за хвостами. Но этот заяц решил подарить букет, чтобы принести радость.\",\n",
    "    \"Ночью в лесу у костра напал страх. Я глядел в огонь, боялся поднять голову. В тишине слышался треск и шёпот, кто-то за спиной шуршал. Рассказ передаёт чувство тревоги и страх перед неизвестным.\",\n",
    "    \"Холодный октябрьский ветер пришёл с севера, из тундры с берегов Печоры. Листобой завывал в печной трубе, шевелил осиновую щепу на крыше, бил и трепал деревья, принося дыхание осени и холода.\"\n",
    "]\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    print(f\"Длинна исходного текста: {len(texts[i])}, Длинна реферата: {len(summaries[i])}, Длинна референса: {len(reference_summaries[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "143dd8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Результаты для пары 1:\n",
      "ROUGE-1: {'r': 0.5483870967741935, 'p': 0.6296296296296297, 'f': 0.5862068915755053}\n",
      "ROUGE-2: {'r': 0.42424242424242425, 'p': 0.5, 'f': 0.45901638847621606}\n",
      "ROUGE-L: {'r': 0.5483870967741935, 'p': 0.6296296296296297, 'f': 0.5862068915755053}\n",
      "\n",
      "Результаты для пары 2:\n",
      "ROUGE-1: {'r': 0.6571428571428571, 'p': 0.8214285714285714, 'f': 0.7301587252204584}\n",
      "ROUGE-2: {'r': 0.4722222222222222, 'p': 0.5666666666666667, 'f': 0.5151515101928377}\n",
      "ROUGE-L: {'r': 0.6285714285714286, 'p': 0.7857142857142857, 'f': 0.6984126934744269}\n",
      "\n",
      "Результаты для пары 3:\n",
      "ROUGE-1: {'r': 0.4838709677419355, 'p': 0.5, 'f': 0.49180327368986837}\n",
      "ROUGE-2: {'r': 0.22857142857142856, 'p': 0.26666666666666666, 'f': 0.24615384118343206}\n",
      "ROUGE-L: {'r': 0.45161290322580644, 'p': 0.4666666666666667, 'f': 0.45901638844396675}\n",
      "\n",
      "Результаты для пары 4:\n",
      "ROUGE-1: {'r': 0.6206896551724138, 'p': 0.6666666666666666, 'f': 0.6428571378635205}\n",
      "ROUGE-2: {'r': 0.41935483870967744, 'p': 0.4482758620689655, 'f': 0.4333333283388889}\n",
      "ROUGE-L: {'r': 0.5862068965517241, 'p': 0.6296296296296297, 'f': 0.6071428521492348}\n",
      "\n",
      "Результаты для пары 5:\n",
      "ROUGE-1: {'r': 0.6428571428571429, 'p': 0.6923076923076923, 'f': 0.6666666616735254}\n",
      "ROUGE-2: {'r': 0.4827586206896552, 'p': 0.5384615384615384, 'f': 0.5090909041057852}\n",
      "ROUGE-L: {'r': 0.6071428571428571, 'p': 0.6538461538461539, 'f': 0.6296296246364884}\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "for i, (summary, reference) in enumerate(zip(summaries, reference_summaries)):\n",
    "    score = rouge.get_scores(summary, reference, avg=False)\n",
    "    print(f\"\\nРезультаты для пары {i+1}:\")\n",
    "    print(f\"ROUGE-1: {score[0]['rouge-1']}\")\n",
    "    print(f\"ROUGE-2: {score[0]['rouge-2']}\")\n",
    "    print(f\"ROUGE-L: {score[0]['rouge-l']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
