{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902c768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca90a732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from razdel import sentenize\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import MBartTokenizer, MBartForConditionalGeneration\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38dfa437",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "model_name = \"cointegrated/rut5-base-absum\" #\"sarahai/ru-sum\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "acdd0f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(30000, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(30000, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(30000, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=30000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# для \"sarahai/ru-sum\" \n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "# model.eval()\n",
    "\n",
    "# для cointegrated/rut5-base-absum\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "# model.eval()\n",
    "\n",
    "\n",
    "# для \"cointegrated/rut5-base-absum\"\n",
    "model_name = \"cointegrated/rut5-base-absum\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c135fab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def abstractive_summary(text, max_symbols=300):\n",
    "#     inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "#     summary_ids = model.generate(\n",
    "#           **inputs,\n",
    "#           max_length=100,\n",
    "#           min_length=60,\n",
    "#           num_beams=30,\n",
    "#           repetition_penalty=3.0,\n",
    "#           length_penalty=1.2,\n",
    "#           no_repeat_ngram_size=10,\n",
    "#           early_stopping=True\n",
    "#       )\n",
    "#     summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "#     return summary[:max_symbols]\n",
    "\n",
    "\n",
    "def improved_abstractive_summary(text, max_symbols=300, device='cuda'):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to('mps')\n",
    "    with torch.no_grad():\n",
    "        summary_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_length=100,          # подогнал под размер 300 на глаз\n",
    "            min_length=60,\n",
    "            num_beams=30,            # взял прямо жирно, но так мне больше нравится\n",
    "            temperature=0.9,         # Для чуть большей вариативности\n",
    "            repetition_penalty=1.5,  # Мягче штрафуем повторения\n",
    "            length_penalty=1.0,      # Нейтральное влияние на длину\n",
    "            no_repeat_ngram_size=3,  \n",
    "            early_stopping=True,\n",
    "            do_sample=True          \n",
    "        )\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary[:max_symbols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40975343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing texts: 100%|██████████| 5/5 [00:43<00:00,  8.71s/it]\n"
     ]
    }
   ],
   "source": [
    "with open(\"ex2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    texts = json.load(f)\n",
    "\n",
    "summaries = []\n",
    "for text in tqdm(texts, desc=\"Summarizing texts\"):\n",
    "    abs_summary = improved_abstractive_summary(text)\n",
    "    summaries.append(abs_summary)\n",
    "\n",
    "with open(\"summaries.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summaries, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec338ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длинна исходного текста: 1054, Длинна реферата: 171, Длинна референса: 271\n",
      "Длинна исходного текста: 1101, Длинна реферата: 179, Длинна референса: 221\n",
      "Длинна исходного текста: 807, Длинна реферата: 191, Длинна референса: 281\n",
      "Длинна исходного текста: 684, Длинна реферата: 150, Длинна референса: 257\n",
      "Длинна исходного текста: 740, Длинна реферата: 200, Длинна референса: 198\n"
     ]
    }
   ],
   "source": [
    "reference_summaries = [\n",
    "    \"В избушке живёт пёс Дик, который любит наблюдать, как курят. Он обжорлив и любит есть рыбью требуху. Однажды автор собрал чернику на болоте и пытался научить Дика уму, показывая, как есть ягоды с куста. Через два дня пёс собрал чернику вокруг избушки, чему автор был рад.\",\n",
    "    \"Скворец Петруша любит подражать голосам и звукам. Поэт сочинял стихи и печатал их в журналах. В песне Петруши есть трели, свисты и рулады, которые звучат как весенние природные явления, принося радость и настроение весны.\",\n",
    "    \"Зайцы обычно не собирают букеты, ведь у каждого из них есть свой цветок — хвост. Но один заяц собрал настоящий букет и не знает, кому его подарить. Лисе и волку цветы не нужны, мишка любит ягоды, а барсук может даже наброситься. Решают отдать букет барсуку и посмотреть, что будет.\",\n",
    "    \"Ночью у костра главного героя охватывает страх. Ему кажется, что кто-то наблюдает из темноты и шепчется вокруг. После громкого крика появляется шум листьев — это шевеление листобоя в кронах деревьев. Рассказ передает атмосферу тревоги и таинственности леса.\",\n",
    "    \"В ночь в печной трубе задул листобой — холодный октябрьский ветер. Форточка была уже раскрыта настежь и полна берёзовых листьев. Листья сбрасывала берёза под окном, а сам хозяин уже скрылся куда-то.\"\n",
    "]\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    print(f\"Длинна исходного текста: {len(texts[i])}, Длинна реферата: {len(summaries[i])}, Длинна референса: {len(reference_summaries[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "143dd8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Результаты для пары 1:\n",
      "ROUGE-1: {'r': 0.3157894736842105, 'p': 0.4, 'f': 0.35294117153979243}\n",
      "ROUGE-2: {'r': 0.09090909090909091, 'p': 0.13333333333333333, 'f': 0.10810810328707107}\n",
      "ROUGE-L: {'r': 0.3157894736842105, 'p': 0.4, 'f': 0.35294117153979243}\n",
      "\n",
      "Результаты для пары 2:\n",
      "ROUGE-1: {'r': 0.45161290322580644, 'p': 0.5185185185185185, 'f': 0.4827586157134364}\n",
      "ROUGE-2: {'r': 0.3333333333333333, 'p': 0.3793103448275862, 'f': 0.354838704698231}\n",
      "ROUGE-L: {'r': 0.45161290322580644, 'p': 0.5185185185185185, 'f': 0.4827586157134364}\n",
      "\n",
      "Результаты для пары 3:\n",
      "ROUGE-1: {'r': 0.22727272727272727, 'p': 0.3225806451612903, 'f': 0.2666666618168889}\n",
      "ROUGE-2: {'r': 0.0625, 'p': 0.0967741935483871, 'f': 0.07594936232014131}\n",
      "ROUGE-L: {'r': 0.22727272727272727, 'p': 0.3225806451612903, 'f': 0.2666666618168889}\n",
      "\n",
      "Результаты для пары 4:\n",
      "ROUGE-1: {'r': 0.16666666666666666, 'p': 0.23076923076923078, 'f': 0.19354838222684714}\n",
      "ROUGE-2: {'r': 0.027777777777777776, 'p': 0.03571428571428571, 'f': 0.03124999507812578}\n",
      "ROUGE-L: {'r': 0.16666666666666666, 'p': 0.23076923076923078, 'f': 0.19354838222684714}\n",
      "\n",
      "Результаты для пары 5:\n",
      "ROUGE-1: {'r': 0.16666666666666666, 'p': 0.19230769230769232, 'f': 0.1785714235969389}\n",
      "ROUGE-2: {'r': 0.06666666666666667, 'p': 0.07692307692307693, 'f': 0.071428566454082}\n",
      "ROUGE-L: {'r': 0.16666666666666666, 'p': 0.19230769230769232, 'f': 0.1785714235969389}\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "for i, (summary, reference) in enumerate(zip(summaries, reference_summaries)):\n",
    "    score = rouge.get_scores(summary, reference, avg=False)\n",
    "    print(f\"\\nРезультаты для пары {i+1}:\")\n",
    "    print(f\"ROUGE-1: {score[0]['rouge-1']}\")\n",
    "    print(f\"ROUGE-2: {score[0]['rouge-2']}\")\n",
    "    print(f\"ROUGE-L: {score[0]['rouge-l']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
